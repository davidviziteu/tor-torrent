import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))

def softmax(z):
    res = np.exp(z)
    return res/np.sum(res)

def relu(z):
    return z * (z > 0)

def relu_derivative(z):
    for idx, val in enumerate(z):
        if val < 0:
            z[idx] = 0
        else:
            z[idx] = 1
    return z


